{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Training Neural Networks with Keras\n",
    "## And universal function approximation\n",
    "## IADS Summer School, 2nd August 2022\n",
    "\n",
    "### Dr Michael Fairbank, University of Essex, UK\n",
    "\n",
    "- Email: m.fairbank@essex.ac.uk\n",
    "- This is Jupyter Notebook 1.2 of the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A simple 1D function\n",
    "\n",
    "- First build some datapoints that represent a simple 1D function, for the sake of a learning example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset for a simple regression problem (1 input 1 output):\n",
    "x_train=np.linspace(-2,2,100).astype(np.float32).reshape(100,1)\n",
    "y_train=(np.sin(x_train*math.pi)*0.3+x_train+-2).astype(np.float32).reshape(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"x_train\",x_train.shape, x_train[0:10])\n",
    "print(\"y_train\",y_train.shape, y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# show training set\n",
    "plt.plot(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build a neural-network model capable of learning this function, from the datapoints\n",
    "- Use Keras to build a 3-layer feed-forward network (i.e. with 2 hidden layers).\n",
    "<img src=\"./images/ffnn_3layers.svg\" alt=\"3-layer FFNN\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define Sequential model with 3 layers\n",
    "model = keras.Sequential(name=\"my_neural_network\")\n",
    "layer1=layers.Dense(10, activation=\"tanh\", input_shape=(1,))\n",
    "model.add(layer1)\n",
    "layer2=layers.Dense(10, activation=\"tanh\")\n",
    "model.add(layer2)\n",
    "layer3=layers.Dense(1)\n",
    "model.add(layer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding the model architecture\n",
    "\n",
    "- The above model print out shows us this is a 3-layer neural network.  \n",
    "\n",
    "- The \"Output Shape\" shows us how many outputs each layer has.  The first dimension is the batch size (which is flexible, hence \"None\"), and the second dimension is the number of outputs for that layer.\n",
    "\n",
    "- We can see from the final layer's output shape how many outputs this network has.\n",
    "\n",
    "- We can see from line 7 of the code how many inputs this network has.\n",
    "\n",
    "**Questions:** \n",
    "\n",
    "1. How many inputs and how many outputs does this neural network have?  **Answer**:\n",
    "2. Why do all of the output shapes start with \"None\"?  **Answer**:\n",
    "3. What is the \"rank\" of all of the output shapes?  **Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding model layers\n",
    "\n",
    "- In this network layer1 and layer2 are called \"hidden layers\", because they are only used for the internal calculation of the network output.\n",
    "\n",
    "- Each layer is parameterised by one or more tensors.  Tensors are just multidimensional arrays of numbers, e.g. a matrix or a vector.\n",
    "    - E.g. a matrix of shape $5 \\times 5$ is a rank-2 tensor of shape=(5,5)\n",
    "\n",
    "- For each Dense layer, there is one weights matrix and one bias vector.  These can be seen below.  \n",
    "\n",
    "- They are initially created with random values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"layer1 weights\",layer1.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Question:\n",
    "\n",
    "1. How many parameters in W and b are there for the first layer? **Answer:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"layer2 weights\",layer2.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"layer3 weights\",layer3.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Questions:**\n",
    "\n",
    "1. How many parameters in W and b are there for the second layer?  **Answer**:\n",
    "\n",
    "2. Do these match what the model.summary() said (on previous code block) **Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Each layer acts as a callable function, and the whole model we have created acts as a callable function.\n",
    "\n",
    "- Layer1 has a weights matrix W (with shape \\[1,10\\]) and a bias vector b (with shape \\[10\\]).  It computes its output $y$ for an input $x$ by $y=tanh(xW+b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Try putting a single input into the layer 1\n",
    "print(layer1(np.array([[4]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can you verify that this matches $y=tanh(xW+b)$?  \n",
    "- **Do this**: Fill in the missing line of code below to help you, and check you get the same output as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x=tf.constant([[4.0]])\n",
    "W=layer1.trainable_weights[0] # This is the weight matrix of shape [1,10]\n",
    "b=layer1.trainable_weights[1] # This is the bias vector of shape [10]\n",
    "print(tf.matmul(x,W))  # TODO fix this line using the tensorflow functions tf.tanh(A) and tf.matmul(A,B) and the tf.add(A,B) functions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that in the final add in the above code, tensorflow used [\"broadcasting\"](https://numpy.org/devdocs/user/theory.broadcasting.html) to allow it to add a rank-2 tensor (a 2d array) to a rank-1 tensor (a 1d array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The whole network acts as a function too.  It just puts the input into the first layer, and then the output of that into the next layer, and so on.\n",
    "\n",
    "\n",
    "**Questions:** \n",
    "\n",
    "1. If the $k$th layer can be written as a function $y=tanh(x.Wk+bk)$, then how could we write the whole network as a single mathematical function?  **Answer:**  (Enter in markdown here):\n",
    "\n",
    "2. Why do we need the tanh functions after every layer?  What would happen if we removed them?  **Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The neural network expects its input to be a rank-2 tensor (i.e. a matrix)\n",
    "- Each row of that matrix corresponds to a different input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Try putting a single input into the whole network\n",
    "print(model(np.array([[4]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Try putting a \"batch\" of 2 input vectors through the network\n",
    "print(model(np.array([[4],[2]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Notice how even though the model function accepts 1 input, it can process two 1d-vector inputs at the same time.  They are processed independently of each other - we see we get the same output now when we push \"4\" though the model as when we pushed \"4\" though on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's put a whole \"batch\" of x values through:\n",
    "print(\"input vectors\", x_train[0:10,:])\n",
    "print(\"output vectors\", model(np.array(x_train[0:10].reshape(10,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot the model's current behaviour:\n",
    "plt.plot(x_train, y_train, label = \"targets\")\n",
    "plt.plot(x_train, model(x_train).numpy(),label=\"model output\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The above graph shows the neural network is not doing what we want it to yet\n",
    "    - because we've just build our network with entirely random weights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the neural network\n",
    "\n",
    "So next we'll \"train\" the network, i.e. change the values of its weights so that its outputs match the target curve.  Note, that by the universal function approximation theorem for neural networks, if we have enough weights and hidden layers, then we can in theory learn any function to arbitrary accuracy.  \n",
    "\n",
    "There is no closed-form solution to this \"training\" problem, so we need to use an iterative numerical method.\n",
    "\n",
    "First we define a loss function which we want to minimise with respect to all of the trainable variables in the neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.01),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanSquaredError()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next we run the iterative procedure.  Here we say we're going to run a full pass through the training set (all of the elements of x_train), 1000 times..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=len(x_train),\n",
    "    epochs=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we see how the neural network's output has (hopefully) improved..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train, label = \"targets\")\n",
    "plt.plot(x_train, model(x_train).numpy(),label=\"model output\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Run the previous 2 cells again to train the network a bit more.  \n",
    "\n",
    "We can see the universal function approximation capability of the neural network in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next we'll view how some of the weights have changed from earlier, by the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"layer1 weights\",layer1.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding the Training Objective, and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These weights have changed - because the training process works by iteratively adjusting the weights to perform gradient descent on the \"loss\" function.  Here we used the Mean Squared Error, so we have minimised\n",
    "$$L=(1/N)\\sum_{k=1}^N (f({x}_k,w)-y_k)^2$$\n",
    "with respect to all of the weights $w$, where $w=$(layer1.traininable_weights, layer2.trainable_weights, layer3.trainable_weights), and where $f$ is the neural network model, and $(x_k, y_k)$ are the $k$th training point's $x$ and (target) $y$ value.\n",
    "\n",
    "We can plot how $L$ decreased over time during training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Saving your network\n",
    "\n",
    "- We can save our final model, and its weights and biases, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.save('saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We can then load it back at a later date with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model2 = keras.models.load_model('saved_model') # just need to give it a folder name here.\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further Challenges\n",
    "\n",
    "If you get time today then:\n",
    "\n",
    "- What happens if we put a tanh activation function into the final layer?  Try it?  What problems do we get for learning this particular dataset?  **Answer:**\n",
    "\n",
    "- How many hidden layers should we have?  Try removing layer 1 and layer 2, so the neural network becomes a simple linear function, and retrain it.  What happens then?  **Answer:**\n",
    "\n",
    "- What will happen to the function approximation capabilities of this network if we increase the number of nodes in each hidden layer?  **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Follow-up Reading\n",
    "\n",
    "- Learn more about the [keras train and evaluate](https://www.tensorflow.org/guide/keras/train_and_evaluate) process.\n",
    "\n",
    "-  For most learning tasks you need a validation set too, and you can use it to check you are not overfitting the data.  See [overfit and underfit](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
