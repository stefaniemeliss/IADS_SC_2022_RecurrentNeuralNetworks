{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A very quick introduction to Automatic Differention and Gradient Descent\n",
    "\n",
    "## Example: using Gradient Descent to solve a Linear Regression Problem\n",
    "\n",
    "## IADS Summer School, 2nd August 2022\n",
    "\n",
    "### Dr Michael Fairbank, University of Essex, UK\n",
    "\n",
    "- Email: m.fairbank@essex.ac.uk\n",
    "- This is Jupyter Notebook 1.1 of the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build a scatter diagram\n",
    "\n",
    "- First build some datapoints for a simple linear regression example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset for a simple regression problem (1 input 1 output):\n",
    "dataset_size=10\n",
    "noise_level=1\n",
    "np.random.seed(1) # Seed the random number generator so the whole class have consistent results\n",
    "x_train=(np.random.rand(dataset_size,1)*5+2).astype(np.float32)\n",
    "y_train=(3*x_train+1+np.random.normal(scale=noise_level, size=(dataset_size,1))).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"x_train\",x_train.shape, x_train)\n",
    "print(\"y_train\",y_train.shape, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# show training set\n",
    "plt.scatter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build a linear regression model\n",
    "- We will build a linear regression model $y=wx+b$ to try to fit the data.  \n",
    "- We don't know what $w$ and $b$ are yet though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w=tf.Variable(0.0, tf.float32)\n",
    "b=tf.Variable(0.0, tf.float32)\n",
    "\n",
    "def model(x):\n",
    "    return x*w+b\n",
    "print(model(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's show how our model depends upon $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w.assign(0.1)\n",
    "print(model(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In the cell below, try to print what the model outputs given input x=0.65.\n",
    "    - **Try This** (Check: we should get 0.065)\n",
    "    \n",
    "- Remember in Jupyter Notebooks you can use \"menu:cell:run cells\" to run the currently highlighted cells.\n",
    "        - the same can be achieved with ctrl+Enter\n",
    "    - Also in Jupyter, in some notebooks you might need to \"Menu:Cell:run all\" to refresh all variables in the correct order\n",
    "        - or even use \"Kernel: Restart and run all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# TODO put some code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Try This**: Change b to 0.2 and evaluate the model given input x=0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# TODO put some code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define a \"loss function\" \n",
    "- To apply gradient descent, we need to define a loss function.\n",
    "- This loss function calculates how well the model is doing\n",
    "- We will manually encode the mean-squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(x,y):\n",
    "    return tf.reduce_mean(tf.square(model(x)-y))\n",
    "print(loss_function(x_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Try this:**\n",
    "    - In the cell below, evaluate the loss function when x=1 and the output is supposedly 4.\n",
    "    - Compare this to the actual model output when x=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# TODO put some code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use Automatic Differentiation\n",
    "\n",
    "- Automatic differention is one of the amazing things about tensorflow, which allows Keras to build and train neural networks very easily.  \n",
    "- It uses \"backpropagation\" to compute the necessary gradients.\n",
    "- Here we'll use it to compute the gradient of the loss function with respect to $w$ and $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "\n",
    "    # Run the forward pass of the layer.\n",
    "    # The operations that the layer applies\n",
    "    # to its inputs are going to be recorded\n",
    "    # on the GradientTape.\n",
    "    L=loss_function(x_train, y_train)\n",
    "print(\"L\",L.numpy())\n",
    "grads = tape.gradient(L, [w,b])\n",
    "print(\"w,b\",w.numpy(),b.numpy())\n",
    "print(\"dL/dw\",grads[0].numpy())\n",
    "print(\"dL/db\",grads[1].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Apply an iteration of \"gradient descent\"\n",
    "If we aim to improve $L$ a tiny bit then we can use the following two updates:\n",
    "- $w \\leftarrow w-\\eta \\frac{\\partial L}{\\partial w}$\n",
    "- $b \\leftarrow b-\\eta \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "where $\\eta>0$ is any small number we choose.\n",
    "\n",
    "This is what is meant by \"gradient descent\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "eta=0.001\n",
    "w.assign(w-eta*grads[0])\n",
    "b.assign(b-eta*grads[1])\n",
    "print(\"w,b\",w.numpy(),b.numpy())\n",
    "L=loss_function(x_train, y_train)\n",
    "print(\"L\",L.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iteratively apply gradient descent\n",
    "\n",
    "- Run the code block below serveral times and see what $w$ and $b$ converge to.  \n",
    "    - In Jupyter, Click ctrl+enter to run a block.\n",
    "- **Extra Challenge:** Try to wrap the code block below into a python \"for loop\" to run 10000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    L=loss_function(x_train, y_train)\n",
    "grads = tape.gradient(L, [w,b])\n",
    "\n",
    "eta=0.001\n",
    "w.assign(w-eta*grads[0])\n",
    "b.assign(b-eta*grads[1])\n",
    "print(\"w,b\",w.numpy(),b.numpy())\n",
    "new_L=tf.reduce_mean(tf.square(model(x_train)-y_train))\n",
    "print(\"new_L\",new_L.numpy(),\". Old L\",L.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Use a built-in optimizer\n",
    "\n",
    "- Instead of hand coding the updates for $w$ and $b$, we can use a built-in optimiser.\n",
    "- For ordinary gradient descent we use \"SGD\" optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "w_history=[] # keep a history for plotting purposes\n",
    "b_history=[]\n",
    "L_history=[]\n",
    "w.assign(0) # We'll restart from scratch in this script\n",
    "b.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "eta=0.001\n",
    "optimizer=keras.optimizers.SGD(eta)\n",
    "for i in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        L=loss_function(x_train, y_train)\n",
    "        \n",
    "    grads = tape.gradient(L, [w,b])\n",
    "    optimizer.apply_gradients(zip(grads, [w,b])) # This line updates w and b with 1 step of gradient descent\n",
    "    \n",
    "    w_history.append(w.numpy()) # append to history for plotting purposes\n",
    "    b_history.append(b.numpy())\n",
    "    L_history.append(L.numpy())\n",
    "print(\"w,b\",w.numpy(),b.numpy())\n",
    "new_L=tf.reduce_mean(tf.square(model(x_train)-y_train))\n",
    "print(\"new_L\",new_L.numpy(),\". Old L\",L.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compare to optimal values for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x_train.reshape(-1),y_train.reshape(-1))\n",
    "print(\"slope\",slope)\n",
    "print(\"intercept\",intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Plot the history graphs to see how well the variables converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=[15, 3])\n",
    "ax1=fig.add_subplot(1,3, 1)\n",
    "ax1.plot(L_history)\n",
    "ax1.set_title(\"L history\")              \n",
    "ax2=fig.add_subplot(1,3, 2)\n",
    "ax2.plot(w_history)\n",
    "ax2.set_title(\"w history\")              \n",
    "ax3=fig.add_subplot(1,3, 3)\n",
    "ax3.plot(b_history)\n",
    "ax3.set_title(\"b history\")              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise:**\n",
    "\n",
    "1. Rerun the gradient descent iterations until the model's $w$ and $b$ parameters match the actual slope and intercept values calculated by stats.linregress\n",
    "2. Increase / decrease the learning rate as you see fit.  \n",
    "\n",
    "**Questions:**\n",
    "1. What happens if eta is too large?  **TODO:** Write answer here\n",
    "2. What happens if eta is too small?  **TODO:** Write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using a Keras \"model\" to hold $w$ and $b$\n",
    "Instead of building our own linear model $y=wx+b$ as above, we can use the in-built keras \"Dense\" layer\n",
    "\n",
    "- We will use a keras \"Dense\" layer with 1 input and 1 output.\n",
    "- This keras dense layer represents a model $y=wx+b$, where $w$ and $b$ are learnable variables.  \n",
    "    - $w$ is called the \"kernel\", and $b$ is called the \"bias\"\n",
    "    - Note that $w$ is a 1*1 matrix, and $b$ is a length-1 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class LinearModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.layer1=layers.Dense(1)\n",
    "    def call(self, x):\n",
    "        return self.layer1(x)\n",
    "    \n",
    "keras_linear_model=LinearModel()\n",
    "keras_linear_model(x_train)\n",
    "print(\"This is the w value\",keras_linear_model.layer1.kernel.numpy())\n",
    "print(\"This is the b value\",keras_linear_model.layer1.bias.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use Keras \"fit\" for the training loop\n",
    "- Keras allows us to quickly define the optimizer to use (SGD), and the loss function to use\n",
    "- It also provides a \"fit\" function which executes the training loop.\n",
    "\n",
    "Run and study the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "keras_linear_model.compile(\n",
    "    optimizer=keras.optimizers.SGD(0.01),  # Optimizer\n",
    "    loss=keras.losses.MeanSquaredError()\n",
    ")\n",
    "history = keras_linear_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=len(x_train),\n",
    "    epochs=10000,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"w\",keras_linear_model.layer1.kernel.numpy(),\"Optimal w:\",slope)\n",
    "print(\"b\",keras_linear_model.layer1.bias.numpy(),\"Optimal b:\",intercept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- We have built this simple model:\n",
    "<img src=\"./images/ffnn_single_layer.svg\" alt=\"1-layer FFNN\" width=\"300\">\n",
    "- We've been introduced to Automatic Differntiation, and gradient descent.\n",
    "- We've also seen the keras Dense layer, the Keras loss functions, optimizer and fit loop\n",
    "\n",
    "- We'll study all of these more in the next workbook and beyond..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
