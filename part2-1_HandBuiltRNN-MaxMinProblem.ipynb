{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to RNNs with Keras\n",
    "## IADS Summer School, 2nd August 2022\n",
    "\n",
    "### Dr Michael Fairbank, University of Essex, UK\n",
    "\n",
    "- Email: m.fairbank@essex.ac.uk\n",
    "- This is Jupyter Notebook 2.1 of the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducing Recurrent Neural Networks (RNNs)\n",
    "\n",
    "A recurrent neural network is a graph with *cycles* in it, i.e. a network with *one or more backward-linking connections*. \n",
    "\n",
    "- A very simple example is:\n",
    "\n",
    "<img src=\"./images/rnn_mike.svg\" alt=\"A basic RNN\" width=\"400\">\n",
    "\n",
    "- They are built for processing *time sequences* of data, $\\{x_t\\}$ for $t=0,1,\\ldots, T-1$.\n",
    "\n",
    "We will build a RNN of the above type this in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A simple Time-Sequence Problem\n",
    "\n",
    "- The challenge is to train a RNN to find the maximum and minimum of a list of numbers.\n",
    "\n",
    "- So for example, given the list of numbers \"3.1,**-2.1**,**4.0**,2.5,1.0\", the RNN must learn to output \"Max=4.0, Min=-2.1\".\n",
    "\n",
    "- Note that we consider this list \"3.1,-2.1,4.0,2.5,1.0\" to be a *time-sequence*.\n",
    "\n",
    "We can generate some training data of this kind using the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "time_sequence_length=4\n",
    "training_set_size=10000\n",
    "x_train=np.random.rand(training_set_size,time_sequence_length) # builds a matrix of random floats in [0,1)\n",
    "x_train=x_train*2-1 # rescales numbers to be between -1 and +1\n",
    "x_train=x_train*(np.random.rand(training_set_size,1)+0.5) # rescales each row so each row has a different range.  Uses broadcasting to make this multiplication work.\n",
    "x_train=x_train+(np.random.rand(training_set_size,1)*2-1) # shifts the mean of each row. Uses broadcasting.\n",
    "y_train=np.stack([np.max(x_train, axis=1),np.min(x_train, axis=1)],axis=1) # This finds the maximum of each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We can check these numpy arrays are doing what they should.  First print sample 0 and its labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"x_train, sample 0\",x_train[0]) # This is the first training list of numbers\n",
    "print(\"y_train, sample 0\",y_train[0]) # This should show the max and min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hopefully we can see that the y values are the max and min of the corresponding list of x-values.\n",
    "\n",
    "- Then see another training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"x_train, sample 1\",x_train[1]) # This is the next training list of numbers\n",
    "print(\"y_train, sample 1\",y_train[1]) # This should show the max and min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding The Shape of Input Tensors for RNN time Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We have created 10000 time sequences, each with 4 time steps, where each step provides exactly 1 scalar input.  Hence we set:\n",
    "    - training_set_size=10000, \n",
    "    - time_sequence_length=4, \n",
    "    - input_vector_length=1\n",
    "\n",
    "- RNNs in Keras expect time sequences to be rank-3 tensors, of shape [training_set_size, time_sequence_length, input_vector_length], as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_vector_length=1\n",
    "x_train=x_train.reshape(training_set_size,time_sequence_length,input_vector_length).astype(np.float32)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We've defined the input shape for this time sequence.  Next we look at the target-output sequence:\n",
    "\n",
    "- This RNN will only make an output once, after the final time step has been inputted.  Hence it is called a \"Many-to-one RNN\".\n",
    "\n",
    "- When the output does come, it will have output_vector_length=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "output_vector_length=2 \n",
    "y_train=y_train.reshape(training_set_size,output_vector_length).astype(np.float32)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build a RNN to process this input tensor\n",
    "\n",
    "We now use Keras to build a recurrent neural network to process a time sequence.\n",
    "<img src=\"./images/rnn_mike.svg\" alt=\"A basic RNN\" width=\"300\">\n",
    "\n",
    "The time sequence must be a rank-3 tensor (3-dimensional array): \n",
    "\n",
    "1. The first dimension is the batch.\n",
    "2. The second is the time sequence length.\n",
    "3. The third is the number of inputs per time step of the sequence.\n",
    "\n",
    "So the shape is [batch, time_sequence_length, num_inputs_per_timestep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The following code allows us to hand build a RNN that takes the sequence of inputs, and loops around it multiplying it by the appropriate weight matrices, and finally spitting out a single output vector (for each training pattern).  \n",
    "\n",
    "- This way of defining the model is more flexible than the previous \"Sequential\" models we built before.   See [Custom Layers and Models](https://www.tensorflow.org/guide/keras/custom_layers_and_models) for further information.\n",
    "\n",
    "-  The main model runs through the \"call\" function, which takes a rank-3 tensor input, and returns the rank-2 output tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "class HandBuiltRNNModel(keras.Model):\n",
    "    def __init__(self, num_hidden_nodes, num_outputs):\n",
    "        super(HandBuiltRNNModel, self).__init__()\n",
    "        #build layers here, and store as class (instance) variables\n",
    "        self.num_hidden_nodes=num_hidden_nodes\n",
    "        self.main_layer=layers.Dense(self.num_hidden_nodes,activation=\"tanh\")\n",
    "        self.output_layer=layers.Dense(num_outputs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # on entry, inputs should be a rank-3 tensor of shape [batch_size, time_sequence_length, input_vector_length]\n",
    "        x=inputs\n",
    "        batch_size=inputs.shape[0]\n",
    "        hidden_recurrent_state=tf.fill([batch_size, self.num_hidden_nodes], 0.0)\n",
    "        for t in range(time_sequence_length):\n",
    "            input_t=inputs[:,t,:] # This extracts one slice of the rank-3 inputs tensor, resulting in a rank-2 tensor\n",
    "            # The next line concatenates 2 rank-2 tensors (side-by-side) into one large tensor\n",
    "            x=tf.concat([input_t,hidden_recurrent_state],axis=1)\n",
    "            hidden_recurrent_state=self.main_layer(x)\n",
    "        # Now hidden_recurrent_state is of shape [batch_size, num_hidden_nodes]\n",
    "        output_tensor=self.output_layer(hidden_recurrent_state)\n",
    "        return output_tensor # this will be of shape [batch_size, output_vector_length]\n",
    "        \n",
    "model_handbuilt_rnn = HandBuiltRNNModel(num_hidden_nodes=20, num_outputs=output_vector_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This hand-built model consumes a tensor of shape \\[training_set_size,time_sequence_length,input_vector_length\\] and outputs one of shape \\[training_set_size,output_vector_length\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Try putting a single input into the network\n",
    "print(model_handbuilt_rnn(x_train[0:1,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Try putting a \"batch\" of 2 input vectors through the network\n",
    "print(model_handbuilt_rnn(x_train[0:2,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Questions:**\n",
    "\n",
    "1. What are the shapes of the tensors x_train[0:1,:,:], x_train[0:2,:,:], x_train[0,:,:]?\n",
    "\n",
    "2. Write some code below to print out these 3 shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# TODO insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To visualise what the network is doing over whole dataset, we'll try the following scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot the model's current behaviour:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_train[:,0],model_handbuilt_rnn(x_train).numpy()[:,0],label=\"max points\")\n",
    "plt.scatter(y_train[:,1],model_handbuilt_rnn(x_train).numpy()[:,1],label=\"min points\")\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- If the RNN is doing what it should then the scatter points should line up on the line $y=x$.\n",
    "\n",
    "- Of course, it's not trained yet, so the points don't line up at all yet!\n",
    "\n",
    "The above graph shows the neural network is not doing what we want it to yet, because we've just built our network with entirely random weights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the neural network\n",
    "\n",
    "So next we'll \"train\" the network, i.e. change the values of its weights so that its outputs match the target curve.\n",
    "\n",
    "Again we use gradient descent, with the gradients computed for us by TensorFlow's automatic-differentiation, using the Keras \"fit\" loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Back-propagation through time (BPTT)\n",
    "- To compute the necessary derivatives here, we would have to imagine unrolling the recurrent loop \"through time\", making a very long unrolled network. \n",
    "\n",
    "<img src=\"./images/rnn_unroll.svg\" alt=\"Showing a RNN unrolled through time\" width=\"700\">\n",
    "\n",
    "- The unrolled network looks like a feed-forward network.\n",
    "- We  can differentiate carefully step-by-step going backwards through this large unrolled network (\"back-propagating\" derivatives again)\n",
    "    - This gradient-finding technique of differentation is known as \"back-propagation through time\" (BPTT).  \n",
    "    - This is all handled internally by Tensforflow and Keras for us.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So we'll use the usual Keras compile and fit method:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_handbuilt_rnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.01),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "history_handbuilt = model_handbuilt_rnn.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=100,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Now the network is trained, the scatter plot should line up nicely along $y=x$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot the model's current behaviour:\n",
    "plt.scatter(y_train[:,0],model_handbuilt_rnn(x_train).numpy()[:,0],label=\"max points\")\n",
    "plt.scatter(y_train[:,1],model_handbuilt_rnn(x_train).numpy()[:,1],label=\"min points\")\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - These points are starting to line up nicely!\n",
    " - We could re-run the previous 3 cells again to train the network a bit more.  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Try putting a single input into the network\n",
    "print(model_handbuilt_rnn(x_train[0:1,:,:]))\n",
    "# And compare to the target min/max output\n",
    "print(y_train[0:1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The above output is meant to show the output of the neural network closely matches the target min and max values.\n",
    "- It's unlikely though for these values to match exactly\n",
    "- We could train the network for longer, or/and use more hidden nodes, to try and improve this match. \n",
    "- But for this demo, we are satisfied if we get the a rough match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_handbuilt.history['mean_absolute_error'])\n",
    "plt.title('Model Training Performance')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using The  Built-in Keras SimpleRNN Layer\n",
    "- The previous example showed how to build a RNN by hand in Keras.  That exercise was just for educational purposes.\n",
    "- Normally we'd use the SimpleRNN keras layer to build a RNN like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The key thing to understand is that the SimpleRNN layer takes in a tensor of shape \\[training_set_size,time_sequence_length,input_vector_length\\] and outputs one of shape \\[training_set_size, num_units\\].  *Even though it looks like an ordinary single layer, when it executes, it is in fact internally executing the whole time loop of length time_sequence_length.*\n",
    "\n",
    "<img src=\"./images/simplernn_layer_network.svg\"  width=\"600\">\n",
    "\n",
    "- Notice how, in the above diagram, the SimpleRNN layer consumes a rank-3 tensor and spits out a rank-2 tensor?  All of the RNN looping behaviour is handled internally by the SimpleRNN layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- So let's start again with building this model using the Keras-layers way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define Sequential model with 3 layers\n",
    "model_keras_rnn = keras.Sequential(name=\"my_keras_rnn_network\")\n",
    "model_keras_rnn.add(layers.SimpleRNN(20, activation='tanh',input_shape=(time_sequence_length,input_vector_length,)))\n",
    "model_keras_rnn.add(layers.Dense(output_vector_length))\n",
    "model_keras_rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As before, this model consumes a tensor of shape \\[training_set_size,time_sequence_length,input_vector_length\\] and outputs one of shape \\[training_set_size,output_vector_length\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Try putting a single input into the network\n",
    "print(model_keras_rnn(x_train[0:1,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Next, we'll train it as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_keras_rnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.01),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")\n",
    "\n",
    "history_keras_rnn = model_keras_rnn.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=100,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot the model's current behaviour:\n",
    "plt.scatter(y_train[:,0],model_keras_rnn(x_train).numpy()[:,0],label=\"max points\")\n",
    "plt.scatter(y_train[:,1],model_keras_rnn(x_train).numpy()[:,1],label=\"min points\")\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_keras_rnn.history['mean_absolute_error'],label=\"keras RNN\")\n",
    "plt.plot(history_handbuilt.history['mean_absolute_error'],label=\"hand-built RNN\")\n",
    "plt.title('Model Training Performance')\n",
    "plt.ylabel('mean absolute error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Conclusion: We can see the SimpleRNN layer is behaving virtually identically to the hand-built RNN we made at the top of this page.\n",
    "\n",
    "- Therefore if you understand the hand-built code above then you understand the Keras SimpleRNN Layer, and you understand the fundamentals of what a RNN is.\n",
    "\n",
    "- If you don't understand this well, then ask me now!  :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- We've seen what a RNN is and how to build one by hand.\n",
    "- We've used the keras layer to build RNNs quickly\n",
    "\n",
    "An important detail to remember is the rank-3 shape of the input tensor to a \"SimpleRNN\" layer, and how that layer will process the full time sequence embedded in that rank-3 tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## To think about next...\n",
    "\n",
    "**Questions to Answer:**\n",
    "\n",
    "1. For this dataset, if you wanted a feed-forward neural network to solve this problem, how many inputs should that network have and how many outputs? **Answer:** TODO\n",
    "\n",
    "2. If we lengthen the list of numbers in each time sequence, think about how the structure of the feed-forward NN will need changing compared to that of the RNN.  Hence which architecture is more flexible at coping with varying-length sequences better?  **Answer:** TODO\n",
    "\n",
    "3. In this notebook, what will happen if we change time_sequence_length from 4 to say 60? \n",
    "    - **Try it** above - change the variable at the top of this notebook, and re-run everything.   \n",
    "    - Performance should drop.  Why?  And what can we do to fix it?..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
