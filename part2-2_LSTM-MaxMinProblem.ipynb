{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to LSTMs with Keras\n",
    "## IADS Summer School, 2nd August 2021\n",
    "\n",
    "### Dr Michael Fairbank, University of Essex, UK\n",
    "\n",
    "- Email: m.fairbank@essex.ac.uk\n",
    "- This is Jupyter Notebook 2.2 of the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Max-Min time sequence (revisited)\n",
    "We'll use the same dataset as before, but this time we'll use a long time_sequence_length (to make it harder).  This will motivate the introduction of LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "time_sequence_length=60 # Longer time sequence than before (to make this problem harder)\n",
    "training_set_size=10000\n",
    "epochs=30\n",
    "x_train=np.random.rand(training_set_size,time_sequence_length) # builds a matrix of random floats in [0,1)\n",
    "x_train=x_train*2-1 # rescales numbers to be between -1 and +1\n",
    "x_train=x_train*(np.random.rand(training_set_size,1)+0.5) # rescales each row so each row has a different range.  Uses broadcasting to make this multiplication work.\n",
    "x_train=x_train+(np.random.rand(training_set_size,1)*2-1) # shifts the mean of each row. Uses broadcasting.\n",
    "y_train=np.stack([np.max(x_train, axis=1),np.min(x_train, axis=1)],axis=1) # This finds the maximum of each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We can check these numpy arrays are doing what they should.  First print sample 0 and its labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"x_train, sample 0\",x_train[0]) # This is the first training list of numbers\n",
    "print(\"y_train, sample 0\",y_train[0]) # This should show the max and min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Reshape the data as required for a Keras RNN:...\n",
    "input_vector_length=1\n",
    "print(x_train.shape)\n",
    "x_train=x_train.reshape(training_set_size,time_sequence_length,input_vector_length).astype(np.float32)\n",
    "print(x_train.shape)\n",
    "\n",
    "output_vector_length=2 \n",
    "print(y_train.shape)\n",
    "y_train=y_train.reshape(training_set_size,output_vector_length).astype(np.float32)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build a LSTM RNN to process this input tensor\n",
    "\n",
    "Now we will use Keras to build a LSTM recurrent neural network to process a time sequence.\n",
    "\n",
    "- LSTMs were created by Sepp Hochreiter and JÃ¼rgen Schmidhuber in 1997, to try to solve the problem of the hidden-vector's information decaying in long time-sequences in RNNs.  Hence the name \"Long Short Term Memory\" (LSTM).\n",
    "\n",
    "- LSTMs are still RNNs - they are not something different!\n",
    "\n",
    "- They use extra \"memory gates\" to make it harder for information to get erased from the recurrent hidden nodes.\n",
    "\n",
    "- Letting information be held onto for longer like this stops learning process from getting stuck.  Hochreiter and Schmidhuber refer to it as solving the \"vanishing gradients\" problem, which refers to solving the fact that learning gradients decay rapidly in deep simpleRNNs.\n",
    "\n",
    "The code below has similarities to the previous hand-built RNN, but with LSTM enhancements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "class HandBuiltLSTMModel(keras.Model):\n",
    "    def __init__(self, num_hidden_nodes,  num_outputs):\n",
    "        super(HandBuiltLSTMModel, self).__init__()\n",
    "        self.num_hidden_nodes=num_hidden_nodes\n",
    "        self.main_layer=layers.Dense(self.num_hidden_nodes*4,activation=None)\n",
    "        self.output_layer=layers.Dense(num_outputs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x=inputs\n",
    "        batch_size=inputs.shape[0]\n",
    "        hidden_h=tf.fill([batch_size, self.num_hidden_nodes], 0.0)\n",
    "        hidden_c=tf.fill([batch_size, self.num_hidden_nodes], 0.0)\n",
    "        for t in range(time_sequence_length):\n",
    "            x=tf.concat([inputs[:,t,:],hidden_h],axis=1)\n",
    "            layer_output=self.main_layer(x)\n",
    "            # The next line splits the layer_output tensor from shape [batch_size,self.num_hidden_nodes*4]\n",
    "            # into 4 tensor variables, each of shape [batch_size,self.num_hidden_nodes]\n",
    "            input_gate, new_value, forget_gate, output_gate = tf.split(value=layer_output, \n",
    "                        num_or_size_splits=4, axis=1)\n",
    "            forget_bias_tensor = 1.0\n",
    "            hidden_c = ((hidden_c * tf.sigmoid(forget_gate+forget_bias_tensor)) + \n",
    "                     (tf.sigmoid(input_gate) * tf.tanh(new_value)))\n",
    "            hidden_h = tf.tanh(hidden_c) * tf.sigmoid(output_gate)\n",
    "        output_tensor=self.output_layer(hidden_h)\n",
    "        return output_tensor\n",
    "\n",
    "model_handbuilt_lstm = HandBuiltLSTMModel(num_hidden_nodes=20, num_outputs=output_vector_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- This hand-built model executes the usual RNN loop, while also implementing the LSTM equations\n",
    "- Note that the LSTM has 4 times as many weights as the simpleRNN - so will run slower per iteration.  See line 9 to see the *4.\n",
    "- **Try this:** Study the LSTM equations (lines of code) above, and see if you can figure out how it is possible for the hidden state $c$ to be updated, and how the hidden state $h$ is a partial view of $c$\n",
    "    - In understanding the LSTM equations, be aware of what the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) is, and what its range is (i.e. what its maximum and minimum output values are).\n",
    "    - Also note that in lines 25-27, the * operator means elementwise multiplication of two tensors.\n",
    "- **Questions**: \n",
    "    - Why is the forget_gate_bias set to 1?  What effect will that have?  **Answer** TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Try putting a single input into the network\n",
    "print(model_handbuilt_lstm(x_train[0:1,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Try putting a \"batch\" of 2 input vectors through the network\n",
    "print(model_handbuilt_lstm(x_train[0:2,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the neural network\n",
    "\n",
    "So next we'll \"train\" the network, in the usual keras way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_handbuilt_lstm.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.01),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")\n",
    "\n",
    "history_handbuilt_lstm = model_handbuilt_lstm.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=100,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Now the network is trained, the scatter plot should line up nicely along $y=x$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot the model's current behaviour:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_train[:,0],model_handbuilt_lstm(x_train).numpy()[:,0],label=\"max points\")\n",
    "plt.scatter(y_train[:,1],model_handbuilt_lstm(x_train).numpy()[:,1],label=\"min points\")\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - The LSTM is solving the problem well even for the long time sequence!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Try putting a single input into the network\n",
    "print(model_handbuilt_lstm(x_train[0:1,:,:]))\n",
    "# And compare to the target min/max output\n",
    "print(y_train[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_handbuilt_lstm.history['mean_absolute_error'])\n",
    "plt.title('Model Training Performance')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Built-in Keras LSTM layer\n",
    "- The previous example showed how to build a LSTM-RNN by hand in Keras.  That exercise was just for educational purposes.\n",
    "- Normally we'd use the LSTM keras layer to build a RNN like this.\n",
    "\n",
    "<img src=\"./images/lstm_layer_network.svg\"  width=\"600\">\n",
    "\n",
    "- The code and implementation is as it was for the SimpleRNN layer in the previous workbook; but we just swap \"SimpleRNN\" for \"LSTM\", in the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define Sequential LSTM model with 3 layers\n",
    "model_keras_lstm = keras.Sequential(name=\"my_keras_lstm_network\")\n",
    "model_keras_lstm.add(layers.LSTM(20, activation='tanh',input_shape=(time_sequence_length,input_vector_length,)))\n",
    "model_keras_lstm.add(layers.Dense(output_vector_length))\n",
    "model_keras_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As before, this model consumes a tensor of shape \\[training_set_size,time_sequence_length,input_vector_length\\] and outputs one of shape \\[training_set_size,output_vector_length\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model_keras_lstm.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.01),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")\n",
    "\n",
    "history_keras_lstm = model_keras_lstm.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=100,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot the model's current behaviour:\n",
    "plt.scatter(y_train[:,0],model_keras_lstm(x_train).numpy()[:,0],label=\"max points\")\n",
    "plt.scatter(y_train[:,1],model_keras_lstm(x_train).numpy()[:,1],label=\"min points\")\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_keras_lstm.history['mean_absolute_error'],label=\"keras LSTM\")\n",
    "plt.plot(history_handbuilt_lstm.history['mean_absolute_error'],label=\"hand-built LSTM\")\n",
    "plt.title('Model Training Performance')\n",
    "plt.ylabel('mean absolute error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Ok, we've shown the capabilities of the hand-built layer and the Keras LSTM layer nicely match.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compare to SimpleRNN\n",
    "- Let's check the performance using the LSTM really beats the SimpleRNN, which we'll rebuild again here for comparison..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define Sequential LSTM model with 3 layers\n",
    "model_keras_simple_rnn = keras.Sequential()\n",
    "model_keras_simple_rnn.add(layers.SimpleRNN(20, activation='tanh',input_shape=(time_sequence_length,input_vector_length,)))\n",
    "model_keras_simple_rnn.add(layers.Dense(output_vector_length))\n",
    "\n",
    "model_keras_simple_rnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.01),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")\n",
    "\n",
    "history_keras_simple_rnn = model_keras_simple_rnn.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=100,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_keras_lstm.history['mean_absolute_error'],label=\"keras LSTM\")\n",
    "plt.plot(history_handbuilt_lstm.history['mean_absolute_error'],label=\"hand-built LSTM\")\n",
    "plt.plot(history_keras_simple_rnn.history['mean_absolute_error'],label=\"Keras SimpleRNN\")\n",
    "plt.title('Model Training Performance')\n",
    "plt.ylabel('mean absolute error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The above graph shows that LSTM really performs a lot better than the SimpleRNN implementation, especially when time_sequence_length >> 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- We've seen how an LSTM can learn longer time sequences more easily than a SimpleRNN can.\n",
    "- We should understand that a LSTM is a RNN (but with sophisticated \"memory gates\" added).\n",
    "- The LSTM's memory gates let it hold on to information for longer than a simple RNN can.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further reading - GRUs\n",
    "\n",
    "- A [Gated Recurrent Unit](https://en.wikipedia.org/wiki/Gated_recurrent_unit) (GRU) is a simpler variant than the LSTM.  It has half the number of weights and roughly the same performance.  \n",
    "- It can easily be implemented in Keras just by changing \"SimpleRNN\" to \"GRU\".\n",
    "- See https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU \n",
    "\n",
    "\n",
    "- Look at the LSTM equations (written as equations instead of code) here https://en.wikipedia.org/wiki/Long_short-term_memory\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Further Exercise\n",
    "- Change the SimpleRNN model above into a GRU model.  Compare its performance to the LSTM model.  **Try This**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
